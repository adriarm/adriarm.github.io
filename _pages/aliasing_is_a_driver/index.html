<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Aliasing is a Driver of Adversarial Attacks</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:title" content="Aliasing is a Driver of Adversarial Attacks" />
<script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>

<script src="lib.js" type="text/javascript"></script>
<script src="popup.js" type="text/javascript"></script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
	TEXT-ALIGN: center
}
</style>

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    margin-bottom: 0px;
    width: 100%;
  }

  h1 {
    font-weight:300;
  }

  div {
    max-width: 95%;
    margin:auto;
    padding: 10px;
  }

  .table-like {
    display: flex;
    flex-wrap: wrap;
    flex-flow: row wrap;
    justify-content: center;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img {
    padding: 0;
    display: block;
    margin: 0 auto;
    /* max-height: 100%;
    max-width: 100%; */
  }

  iframe {
    max-width: 100%;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    max-width: 1100px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>

<body>

<div id="primarycontent">
<center><h1 style="font-size: 225%">Aliasing is a Driver of Adversarial Attacks</h1></center>
<center>
  <div class="table-like" style="justify-content:space-evenly;max-width:880px;margin:auto;">
    <div width="1"></div>
    <div>
      <center>
        <a href="https://adrianrm99.github.io" style="font-size: larger">Adrián Rodríguez-Muñoz</a>
      </center>
      <center>
        MIT CSAIL
      </center>
    </div>
    <div width="1"></div>
    <div>
      <center>
        <a href="https://web.mit.edu/torralba/www" style="font-size: larger">Antonio Torralba</a>
      </center>
      <center>
        MIT CSAIL
      </center>
    </div>
    <div width="1"></div>
  </div>
</center>
<!-- 
<center>
  <img class="result" src="files/simple_example.svg" style="width: 100">
</center> -->

<div class="table-like" style="justify-content:space-evenly;max-width:900px;margin:auto;">
  <center>
    <table>
      <tr>
        <td style="font-size:20px;margin:20px;font-family:monospace">
          <a style="margin:2px" href="https://arxiv.org/abs/2212.11760">[Paper]</a>
        </td>
        <td style="font-size:20px;margin:20px;font-family:monospace">
          <a style="margin:2px" href="https://github.com/adriarm/aliasing_is_a_driver">[Code]</a>
        </td>
      </tr>
    </table>
  </center>
</div>
<center>
<br>

<h2>Abstract</h2>
<div style="font-size:14px; text-align: justify;">
<p>
  Aliasing is a highly important concept in signal processing, as careful consideration of resolution changes is essential in ensuring 
  transmission and processing quality of audio, image, and video. Despite this, up until recently aliasing has received very little 
  consideration in Deep Learning, with all common architectures carelessly sub-sampling without considering aliasing effects. In this work, 
  we investigate the hypothesis that the existence of adversarial perturbations is due in part to aliasing in neural networks. Our ultimate goal is to increase robustness against adversarial attacks using explainable, non-trained, structural changes only, derived from aliasing 
  first principles.
</p>
<p>
  Our contributions are the following. First, we establish a sufficient condition for no aliasing for general image 
  transformations. Next, we study sources of aliasing in common neural network layers, and derive simple modifications from first principles 
  to eliminate or reduce it. Lastly, our experimental results show a solid link between anti-aliasing and adversarial attacks. Simply 
  reducing aliasing already results in more robust classifiers, and combining anti-aliasing with robust training out-performs solo 
  robust training on $L_2$ attacks with none or minimal losses in performance on $L_{\infty}$ attacks.
</p>
</div>


<hr>
<h2>What is aliasing?</h2>
<br>

<table border="0" cellspacing="0" cellpadding="0" width="900">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;">
        <img class="result" src="files/simple_example.svg" style="width: 100%">
    </td>
  </tr>
</table>

<div style="text-align: justify;">
<p>
  The concept of aliasing is intrinsically related to discrete sampling. In layman's terms, the more ”complex” a continuous-domain signal, 
  the finer the sampling needed to properly represent it. Using an insufficiently fine sampling results in visual artifacts that perceptually 
  destroy the original signal; we call this phenomenon ”aliasing”. Consider the example shown in the above figure: the main ”feature” of the 
  signal, the right-to-left diagonals, is inverted by aliasing when sampling at an insufficient rate.
</p>
</div>


<hr>
<h2>Linking aliasing to adversarial examples</h2>
<br>

<table border="0" cellspacing="0" cellpadding="0" width="900">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;">
        <img class="result" src="files/toy_example_2.svg" style="width: 100%">
    </td>
  </tr>
</table>

<div style="text-align: justify;">
<p>
  Our hypothesis is that adversarial attacks work in part by exploiting the phenomenon of aliasing. See the above figure for a simple 
  but enlightening toy example. As we can see, the dirty image is indistinguishable from the original clean image by a human, and 
  yet their outputs are completely different. The culprit for this bizarre effect is the convolution stride (green box) that carelessly 
  down-samples the input. 
</p>
<p>
  An attacker with knowledge
  about it is able to construct a perturbation focused on manipulating the surviving samples (pixels at even rows and
  columns). The discarded samples (pixels at an odd row or column) serve as extra degrees of freedom that can be used
  to make the attack less noticeable and more powerful.
</p>
<p>  
  The behavior of these analytically constructed attacks is remarkably 
  similar to low-amplitude, gradient-driven attacks: they are imperceptible by humans and drastically change the feature maps of a network. 
  It is thus plausible that attacks may be exploiting aliasing.
</p>
</div>


<hr>
<h2>Neural Networks without aliasing</h2>
<br>

<table border="0" cellspacing="0" cellpadding="0" width="900">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;">
        <img class="result" src="files/new_layer_cards_horizontal_alt.svg" style="width: 100%">
    </td>
  </tr>
</table>

<div style="text-align: justify;">
<p>
  To supress aliasing in neural networks, we expand on existing blurring based approaches such as in [1, 2, 3] 
  by using theory to derive the exact blurring strength necessary, which coincides with the experimentally derived strength 
  as was done in [1]. Furthermore, we also introduce the Quantile ReLu anti-aliasing modification, which is a 
  new way of anti-aliasing independent of and synergistic with blurring-based approaches. The above figure shows 
  a summary graphic of the adaptations used in our experiments.
</p>

<p style="font-size:12px;text-align:left">
  [1] Tero Karras, Miika Aittala, Samuli Laine, Erik H ̈ark ̈onen,
  Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-Free
  Generative Adversarial Networks. In Proc. NeurIPS, 2021 <br/>

  [2] Cristina Vasconcelos, Hugo Larochelle, Vincent Dumoulin,
  Rob Romijnders, Nicolas Le Roux, and Ross Goroshin.
  Impact of Aliasing on Generalization in Deep Convolu-
  tional Networks. In Proceedings of the IEEE/CVF In-
  ternational Conference on Computer Vision (ICCV), pages
  10529–10538, Oct. 2021. <br/>

  [3] Richard Zhang. Making Convolutional Networks Shift-
  Invariant Again. In ICML, 2019.
</p>
</div>


<hr>
<h2>How effective is anti-aliasing as a defense?</h2>
<br>

<table border="0" cellspacing="0" cellpadding="0" width="900">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;">
        <img class="result" src="files/accuracy_amplitude.svg" style="width: 100%">
    </td>
  </tr>
</table>

<div style="text-align: justify;">
<p>
  The above figure plots adversarial strength vs accuracy curves for the five defenses against various attacks on each architecture and dataset.
  <ul>
    <li><b style='color:#0000a7;'>Vanilla:</b> No defense.</li>
    <li><b style='color:#008176;'>Initial Blur:</b> Naive initial blur with [1 4 6 4 1]</li>
    <li><b style='color:#eecc16;'>AA(5):</b> Anti-aliasing all five blocks of the network.</li>
    <li><b style='color:#c1272d;'>AT:</b> Adversarial Training with PGD</li>
    <li><b style='color:#000000;'>AT+AA(2):</b> Combining adversarial training with anti-aliasing the first two blocks of the network.</li>
  </ul>
</p>
<p>
  Simple anti-aliasing measures are sufficient by themselves to increase the robustness of networks significantly for 
  low-amplitude and single-step attacks, especially for the $L_2$ variant.
</p>
<p>
  Furthermore, we observe that the AT+AA(2) defense, which combines anti-aliasing with robust training, consistently outperforms the 
  robust training defense AT on $L_2$ attacks, sometimes by a very wide margin such as in Resnet-50+TinyImagenet where both $L_2$ attacks 
  appear completelybeaten, while maintaining statistically equal robustness on $L_\infty$ attacks.
</p>
</div>

<hr>
<h2>Is aliasing the only reason for adversarial attacks?</h2>
<br>

<div style="text-align: justify;">
  <p>
    No, anti-aliasing is not the full picture. Anti-aliasing by itself is not sufficient to reach the level of robustness of
    adversarial training: there is still a significant performance gap for high-amplitude attacks. Combining anti-aliasing with adversarial 
    training, while better than adversarial training alone, is also not $100\%$ robust, as accuracy still decays significantly as a 
    function of epsilon for $L_\infty$ attacks.
  </p>
  <p>
    Instead, the results show that aliasing plays a significant role in the vulnerability of vanilla networks to adversarial attacks. 
    Simply adding anti-aliasing is enough to obtain a significant measure of robustness over the undefended network,
    and combining anti-aliasing with adversarial training yields a more globally robust model over base AT. Indeed, 
    the combined model seems almost completely impervious to FGSM-$L_2$ and PGD-$L_2$.
  </p>
  <p>
    In this work, we prove that aliasing is a driver of adversarial attacks: one vulnerability that attack algorithms leverage to confound neural
    networks. Our conclusion is that anti-aliasing is a component that a successful defense must have.
  </p>
</div>


<hr>
<h2 style="text-align: left; font-weight: normal">Related work</h2>
<p style="font-size:14px; text-align: justify">
  [1] Tero Karras and Miika Aittala and Samuli Laine and Erik H&auml;rk&ouml;nen and Janne Hellsten and Jaakko Lehtinen and Timo Aila. Alias-Free
  Generative Adversarial Networks. In Proc. NeurIPS, 2021 <br/>

  [2] Cristina Vasconcelos, Hugo Larochelle, Vincent Dumoulin,
  Rob Romijnders, Nicolas Le Roux, and Ross Goroshin.
  Impact of Aliasing on Generalization in Deep Convolu-
  tional Networks. In Proceedings of the IEEE/CVF International Conference on 
  Computer Vision (ICCV), pages 10529–10538, Oct. 2021. <br/>

  [3] Richard Zhang. Making Convolutional Networks Shift-
  Invariant Again. In ICML, 2019. <br/>

  [4] Anadi Chaman and Ivan Dokmani&#263. Truly shift-invariant
  convolutional neural networks. 2021 IEEE/CVF Conference
  on Computer Vision and Pattern Recognition (CVPR), pages
  3772–3782, 2021. <br/>

  [5] Alvin Chan, Y. Ong, and Clement Tan. How does frequency
  bias affect the robustness of neural image classifiers against
  common corruption and adversarial perturbations? In IJCAI,
  2022. <br/>

  [6] Julia Grabinski, Janis Keuper, and Margret Keuper. Aliasing coincides 
  with CNNs vulnerability towards adversarial attacks. In The AAAI-22 Workshop 
  on Adversarial Machine Learning and Beyond, 2022, <br/>

  [7] Md Tahmid Hossain, Shyh Wei Teng, Ferdous Sohel, and
  Guojun Lu. Anti-aliasing deep image classifiers using
  novel depth adaptive blurring and activation function. arXiv
  preprint arXiv:2110.00899, 2021. <br/>

  [8] Ant&ocirc;nio H. Ribeiro and Thomas Schon. How convolutional
  neural networks deal with aliasing. ICASSP 2021 - 2021
  IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP), pages 2755–2759, 2021. <br/>

  [9] Yusuke Tsuzuku and Issei Sato. On the structural sensitivity
  of deep convolutional networks to the directions of fourier
  basis functions. 2019 IEEE/CVF Conference on Computer
  Vision and Pattern Recognition (CVPR), pages 51–60, 2019. <br/>

  [10] Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus
  Cubuk, and Justin Gilmer. A fourier perspective on model
  robustness in computer vision. Advances in Neural Informa-
  tion Processing Systems, 32, 2019. <br/>

  [11] X Zou, F Xiao, Z Yu, and YJ Lee. Delving deeper into anti-
  aliasing in convnets. In Proceedings of the British Machine
  Vision Conference (BMVC), 2020. <br/>
</p>
</center>

<br>
<h2 style="text-align: left; font-weight: normal">Bibtex citation</h2>
<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;padding-right: 4em;width: 95%">
<pre style="font-size: 10pt; margin: .3em 0px; text-align: left;">
  @misc{rodriguezmunoz2022driver,
    title = {Aliasing is a Driver of Adversarial Attacks},
    author = {Rodríguez-Muñoz, Adrián and Torralba, Antonio},
    year={2022},
    eprint={2106.05963},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url = {https://arxiv.org/abs/2212.11760},
  }
</pre>
</div>

<br>
<h2 style="text-align: left; font-weight: normal">Acknowledgements</h2>
<p style="font-size:14px; text-align: justify; padding-bottom: 15px; position: relative; top: -10px;">
Experiments were conducted using computation resources from MIT's Supercloud cluster.
</p>

<style type="text/css" media="all">
.page__footer {
  /*float: left;*/
  padding-top: 1em;
  padding-bottom: 0.5em;
  margin-left: 0;
  margin-right: 0;
  width: 100%;
  clear: both;
  /* sticky footer fix start */
  /*position: absolute;*/
  bottom: 0;
  height: auto;
  /* sticky footer fix end */
  margin-top: 3em;
  color: #898c8f;
  background-color: #f2f3f3;
  padding-left: 0em;
  padding-right: 0em;
  max-width: 100%;
}

.page__footer .links {
  margin-left: auto;
  margin-right: auto;
  max-width: 1000px;
  /*padding: 0;*/
}

.page__footer .links .social-icons {
  padding-left: 0;
  text-align: left;
}
</style>

<div class="page__footer">
  <div class="links">
    <ul class="social-icons">
      <li style='display: inline-block; margin-right: 5px; font-style: bold'><strong>Links:</strong></li>
      <li style='display: inline-block; margin-right: 5px; font-style: normal;'><a href="https://accessibility.mit.edu"><i class="fa fa-fw fas fa-universal-access" aria-hidden="true"></i> Accessibility</a></li>
    </ul>
  </div>
</div>

</body>

</html>