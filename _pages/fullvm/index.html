<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Visual memory approaches leverage an explicit database of image neural embeddings, permitting flexible adding and removal of data by editing of the database.
        However, prior work requires a pre-trained embedding model from which it remains difficult to learn or unlearn data.
        By training the embedding model on procedural data, our approach achieves full compartmentalization with respect to all real-world images 
        while retaining strong performance.">
  <meta name="keywords" content="Visual Memory, Procedural Data, Computer Vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Compartmentalizing Knowledge with Procedural Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="styles.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <link rel="stylesheet" href="./static/css/images.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Compartmentalizing Knowledge with Procedural Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://adriarm.github.io">Adrián Rodríguez-Muñoz</a>,</span>
            <span class="author-block">
              <a href="https://mbaradad.github.io">Manel Baradad</a>,</span>
            <span class="author-block">
                <a href="https://web.mit.edu/phillipi/">Phillip Isola</a>,</span>
            <span class="author-block">
              <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">MIT CSAIL</span>
          </div> 
          <!-- <div class="is-size-3 publication-authors">
            <span class="author-block">CVPR 2024</span>
          </div>  -->

          <!-- <div class="is-size-4 publication-authors">
            <span class="author-block">NeurIPS 2023</span>
          </div>  -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
		            <a href="https://arxiv.org/abs/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
             	 </a>   
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/adriarm/fullvm"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
              	<a href="https://github.com/multimodal-interpretability/FIND" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
              	</a>  
              </span> -->
              <!-- <span class="link-block">
		            <a href="https://text-vision-data.csail.mit.edu/web/supplementary.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-bars"></i>
                  </span>
                  <span>More Results</span>
             	 </a>   
              <!-- </span> -->
              <!-- <span class="link-block">
		            <a href="./static/figures/sm.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file"></i>
                  </span>
                  <span>Supplementary</span>
             	 </a>   
              </span> -->
            
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
          </script>
          <p>
            We train representation models with procedural data only, and apply them on visual similarity, classification, and semantic segmentation tasks 
            without further training by using visual memory&mdash;an explicit database of reference image embeddings. Unlike prior work on visual memory, our 
            approach achieves full compartmentalization with respect to all real-world images while retaining strong performance. Compared to a model trained 
            on Places, our procedural model performs within 1% on NIGHTS visual similarity, outperforms by 8% and 15% on CUB200 and Flowers102 fine-grained 
            classification, and is within 10% on  ImageNet-1K classification. It also demonstrates strong zero-shot segmentation, achieving an \(R^2\) on COCO 
            within 10% of the models trained on real data. Finally, we analyze procedural versus real data models, showing that parts of the same object have 
            dissimilar representations in procedural models, resulting in incorrect searches in memory and explaining the remaining performance gap.
          </p>
          <hr>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <a classa="pub_image">
        <img src="./static/images/teaser_v7.png" class="pub_image"/>
      </a>
      <br>
      <div class="content has-text-justified">
        <p><b>Our approach is as follows</b>: first, an embedding model is trained on procedural data generated with OpenGL code using self-supervised learning (SSL). 
          In this stage, unlearning and attribution is difficult, but procedural data is much less exposed to privacy/bias risk and real-world semantics. 
          Next, we use the embedding model on real world tasks using only a visual memory of reference image embeddings, without extra training. 
          When working with real instead of procedural data, there is high privacy/bias risk and real world semantics. 
          However, isolating all real data to only the memory makes efficient data unlearning and privacy analysis possible. 
          The overall system has perfect control over all real world data, while approximating the performance of standard training.</p>
        <hr>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	    <h3 class="title is-4">Classification with visual memory</h3>
    </div>
    <figure>
      <img src="./static/images/visual_memory_row.png"/>
        <figcaption style="font-size: 1.0em; text-align: justify; display: block; padding-top: 19px;">
          Classification with visual memory uses k-nearest neighbours (KNN) \wrt 
          a database of embeddings rather than a parametric classifier (left). This 
          allows efficient adding and removal of data through modification of the database (right).
        </figcaption>
    </figure>
</section>

<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	    <h3 class="title is-4">Limitations of prior approaches</h3>
    </div>
    <figure>
      <img src="./static/images/unlearning.png"/>
        <figcaption style="font-size: 1.0em; text-align: justify; display: block; padding-top: 19px;">
          Prior visual memory approaches can efficiently unlearn
          data in the memory, but not data used to train the embedding model. Our proposal to use <em>procedural data</em> makes the latter case much less
          likely, as procedural data has drastically lower exposure to privacy risk.
        </figcaption>
    </figure>
</section>

<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	    <h3 class="title is-4">Examples of procedural data</h3>
    </div>
    <figure>
      <img src="./static/images/datasets_examples_small.png"/>
        <figcaption style="font-size: 1.0em; text-align: justify; display: block; padding-top: 19px;">
          Examples of procedural data from prior work, our new Masked Shaders: Shaders KML and Shaders KML Mixup, and the real
          datasets Places and ImageNet. Unlike real world data, procedural data is non-realistic and
          is generated via simple code, and thus is much less exposed
          to the privacy or bias risks that motivate unlearning. 
        </figcaption>
    </figure>
</section> -->

<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	    <h3 class="title is-4">Procedurally trained embeddings are effective KNN classifiers of real images</h3>
    </div>
    <div class="box-container">
      <div class="box" style="width: 50%;">
        <figure>
          <img src="./static/images/unlearning.png"/>
            <figcaption style="font-size: 1.0em; text-align: justify; display: block; padding-top: 19px;">
              <b>Performance:</b>
              On fine-grained datasets, procedural data performs better than realistic data with no semantic overlap. When there is semantic overlap, procedural data is within &lt 10%</a>.
            </figcaption>
        </figure>
      </div>
      <div class="box" style="width: 50%;">
        <figure>
          <img src="./static/images/datasets_examples_website.png"/>
            <figcaption style="font-size: 1.0em; text-align: justify; display: block; padding-top: 19px;">
              <b>Performance:</b>
              On fine-grained datasets, procedural data performs better than realistic data with no semantic overlap. When there is semantic overlap, procedural data is within &lt 10%</a>.
            </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	    <h3 class="title is-4">Procedurally trained embeddings are effective KNN classifiers of real images</h3>
    </div>
    <div class="box-container">
      <div class="box">
        <figure>
          <img src="./static/images/classification.png"/>
            <figcaption style="font-size: 1.0em; text-align: justify; display: block; padding-top: 19px;">
              <b>Performance:</b>
              On fine-grained datasets, procedural data performs better than realistic data with no semantic overlap. When there is semantic overlap, procedural data is within &lt 10%</a>.
            </figcaption>
        </figure>
      </div>
      <div class="box" style="padding: 0px">
        <figure>
          <div id="results-carousel" class="carousel results-carousel" style="background-size: 0px">
            <div class="item item-imagenet">
              <img src="./static/images/imagenet_neighbours.png" style="width: 96%">
            </div>
            <div class="item item-cub">
              <img src="./static/images/cub_neighbours.png" style="width: 96%">
            </div>
            <div class="item item-food">
              <img src="./static/images/food_neighbours.png" style="width: 96%">
            </div>
            <div class="item item-flowers">
              <img src="./static/images/flowers_neighbours.png" style="width: 96%">
            </div>
            <!-- <div class="item item-imagenet">
              <img src="./static/images/imagenet_neighbours.png" style="width: 96%">
            </div> -->
          </div>
          <figcaption style="font-size: 1.0em; text-align: justify; display: block; padding-bottom: 42px;">
            <b>Visualization:</b>
            Procedural models can effectively search for perceptually
            similar images on a wide variety of datasets, despite not seeing
            real-world data during training.
          </figcaption>
      </figure>
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
<h2 class="title">Related Work</h2>
<p style="font-size:14px; text-align: justify">
  [1] Wightman, R.: PyTorch Image Models (2019). https://doi.org/10.5281/zenodo.4414861, https://github.com/rwightman/pytorch-image-models <br />

  [2] Liu, C., Dong, Y., Xiang, W., Yang, X., Su, H., Zhu, J., Chen, Y., He, Y., Xue, H., Zheng, S.:
      A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and
      Rethinking (Feb 2023). https://doi.org/10.48550/arXiv.2302.14301, http://arxiv.org/abs/
      2302.14301, arXiv:2302.14301 [cs] <br />

  [3] Xie, C., Tan, M., Gong, B., Yuille, A., Le, Q.V.: Smooth Adversarial Training (Jul 2021),
      http://arxiv.org/abs/2006.14536, arXiv:2006.14536 [cs] <br />

  [4] Simon-Gabriel, C.J., Ollivier, Y., Bottou, L., Schölkopf, B., Lopez-Paz, D.: First-order Adversarial
      Vulnerability of Neural Networks and Input Dimension (Jun 2019), http://arxiv.
      org/abs/1802.01421, arXiv:1802.01421 [cs, stat] <br />

  [5] Ross, A.S., Doshi-Velez, F.: Improving the Adversarial Robustness and Interpretability of Deep
      Neural Networks by Regularizing their Input Gradients (Nov 2017), http://arxiv.org/abs/
      1711.09404, arXiv:1711.09404 [cs] <br />

  [6] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards Deep Learning Models
      Resistant to Adversarial Attacks. In: International Conference on Learning Representations
      (2018), https://arxiv.org/abs/1706.06083 <br />
</p>
</center>
</div>
</section>

<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
  <h2 class="title">BibTeX</h2>
  <pre><code>
    @inproceedings{rodriguezmunoz2024characterizing,
      title={Characterizing model robustness via natural input gradients},
      author={Adrián Rodríguez-Muñoz and Tongzhou Wang and Antonio Torralba},
      booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
      year={2024},
      url={}
    }
  </code></pre>
</div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    Adrián Rodríguez-Muñoz is supported by the LaCaixa fellowship. Tongzhou Wang is supported by the ONR MURI program.
  </div>
  </section>

<!-- </div> -->

	
<!-- <footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content" style="text-align: center;">
          <p>
           
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<style type="text/css" media="all">
  .page__footer {
    /*float: left;*/
    padding-top: 1em;
    padding-bottom: 0.5em;
    margin-left: 0;
    margin-right: 0;
    width: 100%;
    clear: both;
    /* sticky footer fix start */
    /*position: absolute;*/
    bottom: 0;
    height: auto;
    /* sticky footer fix end */
    margin-top: 3em;
    color: #898c8f;
    background-color: #f2f3f3;
    padding-left: 0em;
    padding-right: 0em;
    max-width: 100%;
  }
  
  .page__footer .links {
    margin-left: auto;
    margin-right: auto;
    max-width: 1000px;
    /*padding: 0;*/
  }
  
  .page__footer .links .social-icons {
    padding-left: 0;
    text-align: left;
  }
  </style>
  
  <div class="page__footer">
    <div class="links">
      <ul class="social-icons">
        <li style='display: inline-block; margin-right: 5px; font-style: bold'><strong>Links:</strong></li>
        <li style='display: inline-block; margin-right: 5px; font-style: normal;'><a href="https://accessibility.mit.edu"><i class="fa fa-fw fas fa-universal-access" aria-hidden="true"></i> Accessibility</a></li>
      </ul>
      This website is adapted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies template</a>, which you are free to borrow if you link back to it in the footer.
    </div>
    <br>
  </div>

</body>
</html>