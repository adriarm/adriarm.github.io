<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="What do text models know about the visual world? We systematically evaluate LLMs’ abilities to
generate and recognize visual concepts of increasing complexity and demonstrate how a visual representation learning system can be trained
using models of text.">
  <meta name="keywords" content="LLMs, Vision-Language">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>A Vision Check-up for Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">A Vision Check-up for Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://pratyushasharma.github.io/">Pratyusha Sharma</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://tamarott.github.io/">Tamar Rott Shaham</a><sup>*</sup>,</span>
		        
            <span class="author-block">
              <a href="https://mbaradad.github.io/">Manel Baradad</a>,</span>
            <span class="author-block">
              <a href="https://stephanie-fu.github.io/">Stephanie Fu</a>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/adrian-rodriguez-munoz/">Adrián Rodríguez-Muñoz</a>,</span>
            <span class="author-block">
              <a href="https://shivamduggal4.github.io/">Shivam Duggal</a>,</span>
            <span class="author-block">
              <a href="https://web.mit.edu/phillipi/">Phillip Isola</a>,</span>
            <span class="author-block">
              <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>
            </span>
          </div>
	        * indicates equal contribution
          <div class="is-size-5 publication-authors">
            <span class="author-block">MIT CSAIL</span>
          </div> 
          <div class="is-size-3 publication-authors">
            <span class="author-block">CVPR 2024</span>
          </div> 

          <!-- <div class="is-size-4 publication-authors">
            <span class="author-block">NeurIPS 2023</span>
          </div>  -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
		            <a href="https://arxiv.org/abs/2401.01862"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
             	 </a>   
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2401.01862.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
              	<a href="https://github.com/multimodal-interpretability/FIND" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
              	</a>  
              </span> -->
              <span class="link-block">
		            <a href="https://text-vision-data.csail.mit.edu/web/supplementary.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-bars"></i>
                  </span>
                  <span>More Results</span>
             	 </a>   
              </span>
              <span class="link-block">
		            <a href="./static/figures/sm.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file"></i>
                  </span>
                  <span>Supplementary</span>
             	 </a>   
              </span>
            
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            What does learning to model relationships between
            strings teach Large Language Models (LLMs) about the visual world? 
            We systematically evaluate LLMs' abilities to
            generate and recognize an assortment of visual concepts of
            increasing complexity and then demonstrate how a preliminary 
            visual representation learning system can be trained
            using models of text. As language models lack the ability to
            consume or output visual information as pixels, we use code
            to represent images in our study. Although LLM-generated
            images do not look like natural images, results on image
            generation and the ability of models to correct these generated 
            images indicate that precise modeling of strings can
            teach language models about numerous aspects of the visual world. 
            Furthermore, experiments on self-supervised
            visual representation learning, utilizing images generated
            with text models, highlight the potential to train vision models 
            capable of making semantic assessments of natural images using just LLMs.
          </p>
          <hr>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/figures/main-figure.mp4"
                type="video/mp4">
      </video>
      <br>
      <div class="content has-text-justified">
        <p><b>Vision check-up for LLMs.</b> I. Testing the visual knowledge of Language Models. We suggest a set of tests to check the
          vision abilities of language models, these include (a) the ability to write code that renders complex visual concepts (b) recognizing visual
          concepts from code (c) correcting rendering code with text-only self-feedback. II. We test whether LLMs can generate data to train a
          high-performance vision system that can be used to make semantic judgments on natural images.</p>
        <hr>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	    <h3 class="title is-4">Generation: Drawing with Text</h3>
    </div>
    <div class="content has-text-justified">
      <p>
        We test LLM's abilities to generate visual concepts of increasing complexity via a <b>textual prompt</b>
        → <b>code</b> → <b>image</b> procedure, and find that LLMs can visualize real-world concepts from across the visual hierarchy. 
        LLMs are capable of generating non-trivial visual compositions; the model composes 
        two unrelated concepts (“car shaped cake”), generates visual phenomena
        (“blurred image”), and manages to correctly interpret spatial relations 
        (e.g. “a row of bicycles” arranged horizontally).
      </p>
    </div>
   <figure>
    <!-- <iframe src="./static/figures/dataset.png" width="600" height="500"></iframe> -->
    <img src="./static/figures/dataset.png" />
      <figcaption style="font-size: 0.8em;">
        <b>The Visual Aptitude Dataset.</b> Captions for scenes: (left to right, top to bottom) Chef standing
        next to a counter with jars; Office with leather couch, surrounded by books;
        Row of bicycles; Birthday boy with car shape cake & candles; Black &
        white cat sitting on side of a computer monitor; Couple of men hearding
      </figcaption>
    </figure> 
</section>


<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	    <h3 class="title is-4">Textual feedback: Correcting with Text</h3>
    </div>
    <figure>
      <!-- <iframe src="./static/figures/dataset.pdf" width="600" height="500"></iframe> -->
      <img src="./static/figures/feedback.png" />
        <figcaption style="font-size: 0.8em;">
          <b>Improved visual generation with text feedback.</b> 
          The improvement in the visual generation of models due to feedback is oftentimes gradual, with the addition of a few features at a time over the course of the feedback process. 
        </figcaption>
      </figure> 
      <br>
    <div class="content has-text-justified">
      <p>
        We demonstrate that the visual generation competence of a language model can be improved 
        using text-based corrections. We do this by closing the feedback loop between the LLM and 
        itself. Here, we first use the language model to generate code illustrating a concept. 
        Following that, the model is repeatedly called by conditioning its generation on its 
        previously generated code and prompted to ``improve its generated code''. We find that 
        making such iterative calls to the model results in improved visual depictions.
      </p>
    </div>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/figures/feedback_res.mp4"
                type="video/mp4">
      </video>
  </div>
</section>


<section class="section" style="margin-top: -5px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
	    <h3 class="title is-4">Learning a Vision System from Text</h3>
    </div>
    <div class="content has-text-justified">
      <p>
        We study if LLM-generated images could serve as a data source for pre-training 
        vision models and compare them to synthetically generated and natural images. 
        <!-- We show that the images generated by LLMs are
        useful for training visual backbones, achieving state-of-the-art 
        performance when compared to and used alongside other synthetic datasets. -->
        We show it is possible for models trained entirely on procedurally generated data from LLMs 
        to perform semantic judgments on natural images despite never having seen one before.
      </p>
    </div>
    <div class="content has-text-centered">
      <figure>
        <!-- <iframe src="./static/figures/dataset.pdf" width="600" height="500"></iframe> -->
        <img src="./static/figures/nearest_neighbors.png"  width=75% />
        <figcaption style="font-size: 0.8em;">
        </figcaption>
      </figure> 
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{sharma2024vision,
      title={A Vision Check-up for Language Models},
      author={Sharma, Pratyusha and Rott Shaham, Tamar and Baradad, Manel and Fu, Stephanie and Rodriguez-Munoz, Adrian and Duggal, Shivam and Isola, Phillip and Torralba, Antonio},
      booktitle={arXiv preprint}
      year={2024}
    }
    </code></pre>
  </div>
</section>

	
<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content" style="text-align: center;">
          <p>
            This website is adapted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies template</a>, which you are free to borrow if you link back to it in the footer.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
	


</body>
</html>
